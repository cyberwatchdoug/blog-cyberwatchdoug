{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hi-im-cyberwatchdoug","title":"Hi, I'm CyberWatchDoug","text":"<p>Husband, father, fixer.</p> <p>This is where I share musings on information technology and security.</p> <p>You can also find me on YouTube, GitHub, Bluesky, and LinkedIn.</p>"},{"location":"#recent-posts","title":"Recent Posts:","text":""},{"location":"2024/09/04/custom-domain-using-azure-cli/","title":"Custom Domain Using Azure CLI","text":""},{"location":"2024/09/04/custom-domain-using-azure-cli/#prerequisites","title":"Prerequisites","text":"<ol> <li>Active Azure subscription</li> <li>Azure CLI installed on your computer</li> <li>A purchased domain name that is ready to use</li> </ol>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-1-create-a-resource-group","title":"Step 1: Create a Resource Group","text":"<p>If you don\u2019t already have a resource group created where the DNS zone and/or Static Web App will be stored, then the following will create that for you:</p> <pre><code>az group create --name newResourceGroup --location eastus\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-2-create-the-dns-zone","title":"Step 2: Create the DNS Zone","text":"<p>The DNS Zone in Azure is the data resource that contains the DNS records for a domain name. With a DNS zone you can manage the DNS records for your domain, and point your registrars name servers to the Azure DNS IP\u2019s for your DNS zone.</p> <p>To create a DNS zone for your custom domain:</p> <pre><code>az network dns zone create --resource-group newResourceGroup --name mydomain.com\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-3-retrieve-the-name-servers","title":"Step 3: Retrieve the Name Servers","text":"<p>These will be required for your domain registrars website. Essentially, your domain requests will be pointed to these listed name servers so that Azure DNS handles the DNS queries to your domain name.</p> <pre><code>az network dns zone show --resource-group newResourceGroup --name mydomain.com `\n--query \"nameServers\" --output table\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-4-add-a-cname-record-to-the-dns-zone","title":"Step 4: Add a CNAME Record to the DNS Zone","text":"<p>The CNAME is needed because it maps the alias, such as the subdomain, to the apex or root domain name. In this guide, we are mapping the <code>www</code> subdomain for our Static Web App.</p> <p>First create the cname record:</p> <pre><code>az network dns record-set cname create --resource-group newResourceGroup `\n--zone-name mydomain.com --name www\n</code></pre> <p>Then set the value of the CNAME record:</p> <pre><code>az network dns record-set cname set-record --resource-group newResourceGroup `\n--zone-name mydomain.com --record-set-name www --cname www.mydomain.com\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-5-configure-custom-domain-in-your-static-web-app","title":"Step 5: Configure Custom Domain in your Static Web App","text":"<p>{{&lt; callout &gt;}} If you have not yet created your Static Web App in Azure, check out Deploy a Hugo site to Azure Static Web Apps {{&lt; /callout &gt;}}</p> <p>Now set the custom domain to your Static Web App:</p> <pre><code>az staticwebapp hostname set --resource-group newResourceGroup --name myStaticWebApp `\n--hostname www.mydomain.com\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-6-verify-domain-ownership","title":"Step 6: Verify Domain Ownership","text":"<p>Due to the nature of Azure providing SSL certificates for your Static Web App, there might be times when you\u2019ll need to verify that you do in-fact own your custom domain name. </p> <p>To verify ownership of the domain, you must add a TXT record to the DNS zone, which is provided by Azure. I know this may sound confusing, since it\u2019s the same place we are configuring but hold steady here.</p> <p>Retrieve the verification token:</p> <pre><code>az staticwebapp hostname show --resource-group newResourceGroup --name myStaticWebApp `\n--hostname www.mydomain.com --query \"validationToken\"\n</code></pre> <p>Then add the TXT record to the DNS zone:</p> <pre><code>az network dns record-set txt add-record --resource-group newResourceGroup \\\n--zone-name mydomain.com --record-set-name asuid --value \"your_validationToken\"\n</code></pre>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#step-7-verify-dns-propagation","title":"Step 7: Verify DNS Propagation","text":"<p>Use tools such as <code>nslookup</code> or <code>dig</code> or online DNS resolvers/checkers to verify the DNS changes. This process can take some time to complete after changes are made to your DNS records.</p>"},{"location":"2024/09/04/custom-domain-using-azure-cli/#resources","title":"Resources","text":"<p>My favorite place to learn more would be tool references, such as the Azure CLI A-Z Reference list</p>"},{"location":"2025/07/28/automating-resilient-backups-in-my-homelab-with-ansible-and-paperless-ngx/","title":"Automating resilient backups in my homelab with Ansible and Paperless-ngx","text":"<p>As my data consumption and storage needs grow - both at work and at home - reliable automation becomes non-negotiable. Over the past week, I have invested much needed time to my homelab_ansible GitHub repository; focusing on crafing a robust backup playbook for my paperless-ngx deployment. My goal? A backup workflow I trust to safeguard my documents, regardless of infrastructure or underlying OS.</p>"},{"location":"2025/07/28/automating-resilient-backups-in-my-homelab-with-ansible-and-paperless-ngx/#getting-started","title":"Getting started","text":"<p>I kicked things off by creating a new playbook that centered around the usage of the document_exporter CLI tool included with paperless-ngx. With GitHub Copilot lending a subtle hand, the scaffolding came together rather quickly. I did need to tweak what modules were initially suggested for various sections, such as shell vs command. However, secrets management - always a sticking point in cybersecurity -- called for some extra attention. I created two Ansible Vaults: one for the SMB credentials, and the other for a GPG encryption passphrase. Both of these I initially tested with the <code>-J</code> vault-password prompt.</p> <p>But you quickly realize after a few tests, that an interactive password prompt is cumbersom and not automation! My next step was shifting the vault keys outside of the the repo, updating my .gitignore, and templating the vault files so only example-vault.yml ships with my repo to GitHub. These are small but critical details for clean and secure source control and future use.</p> <p>Building this playbook out, I also took the opportunity to make it OS-agnostic - supporting both apt and dnf based distros out of the box. I have both Debian and RedHat environments, and being able to redeploy paperless-ngx to either is future-proofing in action. The playbook checks for and installs utilities as needed for SMB, GPG, and CIFS; then runs the export, encrypts the zip file created, and copies the encrypted file to my SMB share.</p>"},{"location":"2025/07/28/automating-resilient-backups-in-my-homelab-with-ansible-and-paperless-ngx/#unprivileged-lxc-snag","title":"Unprivileged LXC snag","text":"<p>One snag when I set out copying to my SMB share was my original attempt at mounting the SMB share locally. My paperless-ngx is running on a Proxmox LXC which is an unprivileged container. These don't play well with mount operations! So instead of mounting the SMB share, I pivoted to using smbclient and the build in put option.</p>"},{"location":"2025/07/28/automating-resilient-backups-in-my-homelab-with-ansible-and-paperless-ngx/#next-steps","title":"Next steps","text":"<p>I'm not stopping here. \"It runs\" is never good enough for a highly analytical and must-constantly-be-improving mindset individual!</p> <p>What's on the roadmap currently:</p> <ul> <li>Integrity importance: file checksums before deleting the original on the host</li> <li>Cloud insurances: automating offsite copies to Azure or Google Drive</li> <li>Smarter retention: Daily backups, plus weekly, monthly, yearly roolups - pending a deeper look at storage overhead.</li> <li>Archive strategies: Keeping what's relevant in paperless-ngx while balancing archived data and backups.</li> </ul> <p>Why does all this matter? Because homelabs are real-world playgrounds for the kind of reliability and automation every security engineer needs. By treating my own data with discipline -- automation, encryption, secrets storage, cross-platform compatibility -- I practice the skills (and the mindset) I want to bring everywhere.</p> <p></p>"},{"location":"2025/08/05/icam-fundamentals-and-definitions/","title":"ICAM fundamentals and definitions","text":"<p>Identity, Credential, and Access Management, commonly referred to as ICAM, marks a fundamental evolution from the traditional approaches to access control in organizations.</p> <p>What do traditional access controls look like? In simple terms, it was straightforward: usernames and passwords. Physical access secured through ID cards or PINs; system and resource access granted through basic logins. Most of this managed manually by IT or security staff. And unless someone forgot their password or hit a forced password reset deadline, accounts and permissions were rarely reviewed or updated!</p> <p>It's astonishing! Such methods were once (and in some cases, still are) the norm!</p> <p>But organizations today face far more complex challenges. Let's take a closer look.</p>"},{"location":"2025/08/05/icam-fundamentals-and-definitions/#the-rise-of-icam","title":"The rise of ICAM","text":"<p>What ICAM represents therefore, is not just an upgrade but a comprehensive rethink of how identities and access are managed. ICAM integrates several modern security principles:</p> <ul> <li>Holistic Integration: Unifies the management of both physical and digital access; a coordinated approach for identities and systems.</li> <li>Automated Lifecycle: Replace manual processes with automated workflows for onboarding, changes, offboarding, and reviews to minimize errors and delays.</li> <li>Zero Trust Architecture: Shift the focus from trusting everything inside the perimeter to continuous verification: never trust, always verify.</li> <li>Risk-Based Access: Grant access dynamically, factoring in context, attributes, and risk instead of relying on static, outdated roles.</li> <li>Federation &amp; Interoperability: Seamless and secure sharing of identity data across organizational and external boundaries; essential for partnerships and cloud services.</li> </ul> <p>This evolution is necessary! Because modern environments add complexity:</p> <ul> <li>Cloud and hybrid IT environments</li> <li>Remote and flexible workforces</li> <li>Supply chain and third-party access</li> <li>Strict regulatory compliance</li> <li>Sophisticated and evolving cyber threats</li> </ul> <p>An effective ICAM strategy addresses all of these and provides for both security and agility in the organization.</p>"},{"location":"2025/08/05/icam-fundamentals-and-definitions/#key-components-of-icam","title":"Key components of ICAM","text":"<p>Let's break down the core elements of ICAM further:</p> <p>Identity Management</p> <p>Focuses on the creation and oversight of digital identities, which represent people, devices, or services. A crucial part of this is identity proofing, which verifies that a claimed identity actually matches an individual or entity using a trusted source. Throughout its lifecycle, each identity is carefully managed, covering all stages beginning with initial creation and onboarding, through role changes or adjustments, and finally to eventual deactivation or deletion when the identity is no longer needed.</p> <p>Credential Management</p> <p>Involves selecting and handling the authentication methods used to verify an identity, whether its passwords, multi-factor tokens, digital certificates, or similar technologies. The lifecycle of the credentials is actively managed as well, using processes to create, update, and securely remove them regularly. Attestation plays a critical role by ensuring that issued credentials remain trustworthy and have not been compromised. This maintains the integrity of the overall system.</p> <p>Access Management</p> <p>Determines what resources and actions each authenticated identity can access within a system (known as authorization). It also covers provisioning, which the granting and revoking of access rights as needed. This is based on established business rules and roles. To ensure security and compliance, systems continually monitor and control which active sessions are open, while also maintaining comprehensive audits that log access attempts and activity. This provides essential records for both regulatory needs and incident investigations (when necessary).</p>"},{"location":"2025/08/05/icam-fundamentals-and-definitions/#putting-it-all-together","title":"Putting it all together","text":"<p>ICAM represents a modern, unified, and comprehensive approach to managing digital identities and access. It isn't just about tools! While traditional methods focused on simple username/password combinations, ICAM delivers:</p> <ul> <li>A unified framework for managing identities</li> <li>Centralized and consistent identity management across all systems and resources</li> <li>Automated processes that reduce human error</li> <li>Risk-based decisions which adapt to changing threats in real-time</li> <li>Robust audit trails for oversight, incident response, and compliance</li> </ul> <p>The key to successful ICAM implementation lies in understanding the fundamentals and applying them systematically across your organization. As cyber threats evolve utilizing AI-driven methods, ICAM will continue to be a critical component of any security and zero trust strategies.</p> <p></p>"},{"location":"2025/08/25/probabilistic-vs-deterministic-identity-security/","title":"Probabilistic vs deterministic identity security","text":"<p>One of the first things to understand when comparing probabilistic and deterministic approaches to identity security is how each operates. This is especially true in environments that rely on user and behavior analytics (UEBA). A UEBA environment will much more commonly rely on probabilistic methods, as the name implies.</p> <p>Let's break these approaches down and explore how they can work together.</p>"},{"location":"2025/08/25/probabilistic-vs-deterministic-identity-security/#probabilistic-approach","title":"Probabilistic approach","text":"<p>Common methods include inferring legitimacy based on behavioral patterns and statistical likelihoods to make assumptions instead of direct confirmations. Sounds a bit like UEBA in practice, right? This approach can be especially valuable for broad threat detection and anomaly alerting. Oftentimes, it can operate effectively even when user data is not complete or ambiguous. But there are weaknesses, with the most glaring one being that attackers can mimic legitimate behaviors using AI or deepfakes along with stolen credentials thereby evading detection.</p> <p>Accuracy can be characterized as moderate due to the heavy reliance on statistical likelihoods and not certainties. Ease of configuration and use is more on the complex side, again because of the dependency on large-scale analytics, ML, and behavioral baselines.</p> <p>Probabilistic methods are useful for spotting unusual activity at scale, but they can never give you full assurance that the person behind the screen is truly who they claim to be.</p>"},{"location":"2025/08/25/probabilistic-vs-deterministic-identity-security/#deterministic-approach","title":"Deterministic approach","text":"<p>Unlike statistical likelihoods, deterministic instead offers binary certainty to its approach through verifying immutable identifiers such as cryptographic keys, biometric matches, and device-specific identifiers. It excels in accuracy and should be the preferred approach used in industries that demand zero tolerance for security lapses (i.e. banking or healthcare). </p> <p>But a major weakness (for now anyways) is that of coverage. It requires robust and high-quality data for every identity interaction in order to make determinations. And if the environment presents the solution with incomplete, anonymous, or highly distributed data sets, then those determinations will either be grossly innacurate or fail to be attained.</p> <p>Put simply: deterministic methods deliver higher confidence, but they require strong infrastructure and reliable data to work.</p> <p>If robust data is achieved, then this approach offers a high level of accuracy with direct, verifiable proof of determinations. Additionally, ease of configuration and use is simpler in general, but again requires strong data verification.</p>"},{"location":"2025/08/25/probabilistic-vs-deterministic-identity-security/#make-ueba-more-deterministic","title":"Make UEBA more deterministic","text":"<p>As pointed out above, UEBA typically relies heavily on ML and analytics to baseline normal behaviors. After which, it can begin to spot anomalies, assign risk scores and trigger alerts. It is inherently probabilistic. However, it does a great job at reducing false positives and providing context-aware alerts. But falling short on identity confirmation with absolute certainty.</p> <p>So how do we improve upon UEBA and increase deterministic processes? There are several ways, all of which start at the organization's strategic vision and goals level.</p> <p>For leaders, the practical takeaway is that you don't need to choose between probabilistic and deterministic methods. The most reslient strategy blends both.</p> <p>Here are four ways to bring more determinism into UEBA:</p> <ol> <li> <p>Start with integrating deterministic identity verification into authentication workflows.</p> <ul> <li>employ cryptographic authentication such as passkeys, smart cards, or PIVs.</li> <li>bind authentication to registered and trusted devices.</li> <li>require biometric verification</li> </ul> </li> <li> <p>Layer deterministic controls within existing UEBA</p> <ul> <li>Use deterministic identity proofing at login and during privileged escalation events</li> <li>continue probabilistic monitoring for ongoing risk assessment</li> </ul> </li> <li> <p>Integrate continuous identity assurance</p> <ul> <li>Adapt your risk-based policies to apply deterministic checks when a UEBA risk score increases.</li> <li>Example adaptations could be triggering cryptographic or biometric re-authentication.</li> </ul> </li> <li> <p>Ensure and enforce strict identity verification at account creation and every privileged event</p> <ul> <li>require proof at regular intervals but especially after suspicious activities and privileged escalation events.</li> </ul> </li> </ol>"},{"location":"2025/08/25/probabilistic-vs-deterministic-identity-security/#final-thoughts","title":"Final thoughts","text":"<p>Probabilistic approaches like UEBA are powerful for broad monitoring, but on their own, they leave gaps and can miss targeted identity attacks. By weaving deterministic identity verification into workflows, especially at login and during privilege escalation events, organizations can close those gaps and materially reduce impersonation risk. The most resilient organizations combine the two.</p> <p>In short: probabilistic for breadth, deterministic for certainty. Together they strengthen identity security across the enterprise.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/","title":"How to Sync an Obsidian Vault Between Windows 11 and Linux (Debian LXC) Using Syncthing for n8n Automations","text":""},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#why-i-needed-a-cross-platform-obsidian-sync-setup","title":"Why I needed a Cross-Platform Obsidian Sync Setup","text":"<p>Keeping an Obsidian vault synced between Windows and Linux can be tricky if you don\u2019t want to rely on third\u2011party cloud services like Dropbox or Google Drive. I ran into this challenge while trying to keep my various vaults consistent across my Windows 11 desktop (where I do most of my vault work) and a Debian Linux LXC running on Proxmox, where I host my automation tools. My goal wasn\u2019t just simple note syncing, I wanted to unlock powerful n8n workflows that process and automate my notes directly from the server.</p> <p>After brainstorming a few options, I landed on Syncthing as the perfect solution. It\u2019s lightweight, private, and runs quietly in the background without any dependency on external services. Even better, I figured out how to start Syncthing automatically on Windows 11 without the annoying terminal popup, while also configuring a persistent Syncthing systemd service inside my Debian LXC. In this post, I\u2019ll walk you through exactly how I set it up and how it powers my Obsidian + n8n automation workflow.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#setting-up-the-environment","title":"Setting Up the Environment","text":""},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#my-setup-windows-11-debian-lxc-on-proxmox-and-obsidian","title":"My Setup: Windows 11, Debian LXC on Proxmox, and Obsidian","text":"<p>My environment is nothing fancy. A simple Windows 11 Pro desktop PC, and my Proxmox device running on old hardware. I have a few LXC's on the device, so my one for n8n isn't alone. Each is accessible through my firewall even though I have my network segmentated with VLANs.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#why-syncthing-instead-of-cloud-based-sync-options","title":"Why Syncthing Instead of Cloud-Based Sync Options","text":"<p>I chose to use Syncthing instead of alternative options for the simple reason of self-hosted and private. Plus who doesn't like having a bit of fun with configuration and system administration tasks? Additionally, I wanted a stand-alone Syncthing setup, not just using a community plugin within Obsidian. If I'm going to use a tool I want to enact full control with that tool for how I need it.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#installing-and-configuring-syncthing-on-windows-11","title":"Installing and Configuring Syncthing on Windows 11","text":""},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#step-1-download-and-install-syncthing-on-windows-11","title":"Step 1: Download and Install Syncthing on Windows 11","text":"<p>Head on over to the official downloads page here: syncthing.net/downloads/. I chose the Base Syncthing, which is just a zip file with everything needed to run it on my Windows 11 PC. All you need to do after downloading is unzip the folder.</p> <p>For me, I took an additional step and copied over the syncthing folder to the following location <code>C:\\opt\\syncthing\\</code> as that is the folder structure I'm used to for installing software.</p> <p>I should point out, I use the term \"install\" loosely here, because we aren't actually installing anything. Syncthing is just an executable that we run after it is download.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#step-2-run-syncthing-automatically-at-windows-startup","title":"Step 2: Run Syncthing Automatically at Windows Startup","text":"<p>Per the documentation, there are multiple ways to achieve running Syncthing automatically at startup in Windows 11. I took the approach of creating a task in Task Scheduler.</p> <p>Here is the rather simple way of doing this using PowerShell:</p> <pre><code>&lt;#\n.SYNOPSIS\n    Creates or updates a scheduled task to run Syncthing at user login.\n#&gt;\n\nparam(\n    [string]$ExePath = \"Powershell.exe\",\n    [string]$TaskName = \"SyncthingStartup\"\n)\n\ntry {\n    if (-not (Test-Path $ExePath)) {\n        Write-Error \"Syncthing executable not found at $ExePath\"\n        exit 1\n    }\n\n    $userName = \"$env:USERNAME\"\n    $action = New-ScheduledTaskAction -Execute $ExePath -Argument \"-WindowStyle Hidden -Command \"Start-Process 'C:\\opt\\syncthing\\syncthing.exe' -ArgumentList '--no-console'\"\"\n    $trigger = New-ScheduledTaskTrigger -AtLogOn\n    $principal = New-ScheduledTaskPrincipal -UserId $userName -LogonType Interactive\n    $settings = New-ScheduledTaskSettingsSet -AllowStartIfOnBatteries -DontStopIfGoingOnBatteries -StartWhenAvailable\n\n    # Remove existing task if it exists\n    if (Get-ScheduledTask -TaskName $TaskName -ErrorAction SilentlyContinue) {\n        Unregister-ScheduledTask -TaskName $TaskName -Confirm:$false\n    }\n\n    Register-ScheduledTask -TaskName $TaskName -Action $action -Trigger $trigger -Principal $principal -Settings $settings -Description \"Start Syncthing at user login\" -Force\n    Write-Output \"Scheduled task '$TaskName' created successfully.\"\n}\ncatch {\n    Write-Error \"Failed to create scheduled task: $_\"\n}\n</code></pre> <p>Running the Synchthing executable by way of <code>Powershell.exe</code> helped me permanently solve the issue of having a Terminal popup when it ran. This is because the option <code>--no-console</code> is not reliable on Windows 11, and may still cause the launcher to open a console window.</p> <p>The other part to fixing this was to set your Default terminal application to <code>Windows Control Host</code> rather then the default <code>Let Windows decide</code>.</p> <p>Doing those 2 things made the task schedule work without a Terminal window persistently being opened.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#installing-and-running-syncthing-on-debian-lxc-proxmox","title":"Installing and Running Syncthing on Debian LXC (Proxmox)","text":"<p>There's a setup guide on https://apt.syncthing.net/ you can follow for this step. I'll briefly detail it below.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#step-1-add-official-syncthing-repository-on-debian","title":"Step 1: Add Official Syncthing Repository on Debian","text":"<p>Add the release PGP key and the stable-v2 channel to your apt sources:</p> <pre><code>sudo mkdir -p /etc/apt/keyrings\nsudo curl -L -o /etc/apt/keyrings/syncthing-archive-keyring.gpg https://syncthing.net/release-key.gpg\n\necho \"deb [signed-by=/etc/apt/keyrings/syncthing-archive-keyring.gpg] https://apt.syncthing.net/ syncthing stable-v2\" | sudo tee /etc/apt/sources.list.d/syncthing.list\n</code></pre>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#step-2-install-syncthing-with-apt","title":"Step 2: Install Syncthing with apt","text":"<p>To install, perform an <code>sudo apt update</code> and then <code>sudo apt install syncthing</code></p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#step-3-running-syncthing-as-systemd-service-in-debian","title":"Step 3: Running Syncthing as systemd Service in Debian","text":"<p>There is a keen distinction to understand here. You can run Syncthing as a systemd service as either a User Service, or as a System Service.</p> <p>Running as a User Service ensures that Syncthing only starts after the user has logged into the system.</p> <p>Running as a System Service ensure that Syncthing starts at system startup.</p> <p>For my setup, I need the latter and did this setup to enable and run Syncthing as a System Service for my user:</p> <pre><code>sudo systemctl enable syncthing@doug.service\nsudo systemctl start syncthing@doug.service\n</code></pre> <p>replace <code>doug</code> with your username in this case. The reason we perform the system service execution this way is because Syncthing is setup and configured per user, not per system. That's why we don't just run <code>systemctl start syncthing.service</code> here.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#connecting-devices-and-syncing-the-obsidian-vault","title":"Connecting Devices and Syncing the Obsidian Vault","text":""},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#finding-and-adding-syncthing-device-ids","title":"Finding and Adding Syncthing Device IDs","text":"<p>Syncthing is great for security in that it requires you to perform the setup on both devices that you want to communicate and sync files/folders with. This starts with configuring the Device IDs for each.</p> <p>This means I would need the Device ID for my Windows 11 PC, and for my Debian LXC. You can find each easily by just running the <code>syncthing</code> executable on each device. In Windows 11, you can just double click on the <code>syncthing.exe</code> executable and it will open up in the Terminal. What you are looking for is the line with <code>INFO: My ID:</code> and what follows would be the Device ID. Copy this and note which device it is for.</p> <p>You can do the same on Debian with just the <code>syncthing</code> command at the CLI. This too will show a similar line to above with the Device ID.</p> <p>For Windows 11, with the <code>syncthing.exe</code> still running, open up your browser to the following address: <code>http://localhost:8384</code>. The web GUI runs by default and this is my chosen method in Windows 11 since I'll be doing most of the configuration here going forward. Although I would like to automate these steps in the future so using the CLI is on the horizon for me (more on that to come).</p> <p>In the bottom right just click the <code>+ Add Remote Device</code> button, type in a name and paste in the Device ID for your Debian LXC. It won't actually connect successfully until we do the same on the Debian LXC. Remember, we have to configure each device!</p> <p>For the Debian LXC, I don't have access to any GUI, so it's CLI time! Here is the flow I did to get things configured:</p> <pre><code># Add the Windows 11 Device using the Device ID:\nsyncthing cli config devices add --device-id &lt;WIN11_DEVICE_ID&gt;\n</code></pre> <p>If you switch back to your Windows 11 and the Syncthing GUI, you should now see the remote device connected. If not, then you'll have a bit of logs to dig through!</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#sharing-the-obsidian-vault-folder-from-windows-to-linux","title":"Sharing the Obsidian Vault Folder from Windows to Linux","text":"<p>Before sharing my Obsidian Vault folder from Windows to the Debian LXC, you'll need to check to ensure the default folder path is where you want it to be. In my case I wanted the default folder path to be in <code>/opt/syncthing/</code> and not the default <code>~</code>.</p> <p>To do this, open up the configuration file in your favorite editor (vim in my case) and do the following:</p> <pre><code>vim ~/.local/state/syncthing/config.xml\n\n# Search for &lt;defaults&gt;, first press /\n/&lt;defaults&gt;\n\n# make sure the 'path=\"~\"' is the location you want for default folder paths\n</code></pre> <p>So my default folder setting looks like the following now:</p> <pre><code>&lt;folder id=\"\" label=\"\" path=\"/opt/syncthing/\" type=...&gt;\n</code></pre> <p>The next thing was to adjust the <code>autoAcceptFolders</code> setting. I went back into the configuration file on my Debian LXC, found the location for my Windows 11 Device (search for the Device ID), and changed the value for <code>&lt;autoAcceptFolders&gt;&lt;/autoAcceptFolders&gt;</code> from <code>false</code> to <code>true</code>. This file is located in <code>~/.local/state/syncthing/config.xml</code></p> <p>Now all I needed to do was add the folder from the GUI on my Windows 11 PC, and it would automatically get added and synced to my Debian LXC.</p> <p>Doing this in the GUI is pretty straighforward. Select <code>+Add Folder</code> on the left, type a folder label, and specify the folder path. Then select the \"Sharing\" tab at the top, and click the box next to the device you want to share the folder with.</p> <p>If everything was configured correctly on both devices, you should now see your shared folder from Windows 11 on your Debian LXC.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#handling-the-obsidian-folder-and-ignore-patterns","title":"Handling the '.obsidian' Folder and Ignore Patterns","text":"<p>Since I won't be using my Obsidian Vaults on my Debian LXC, I do not need to sync the <code>.obsidian</code> folder. This avoids syncing my UI layout, workspace state, community plugin settings, and several other things that would be important if I planned on opening and using the vault on that device.</p> <p>But since I'm not, I'm adding the ignore pattern <code>.obsidian/</code> to my <code>.stignore</code>. You can also do this in the GUI, when editing the folder, under the \"Ignore Patterns\" tab.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#verifying-syncthing-sync-works","title":"Verifying Syncthing Sync Works","text":"<p>To verify, I used a combination of seeing what state the GUI displayed for both the Folder being shared, and the Device. Both should say \"Up to Date\".</p> <p>You can also check your sync location on Debian LXC and verify that your obsidian vault is there now.</p> <p>An additional step to test would be creating a new file on each device and confirming that is appears on the other device.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#testing-file-sync-between-windows-11-and-debian-lxc","title":"Testing File Sync Between Windows 11 and Debian LXC","text":""},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#common-issues","title":"Common Issues","text":"<p>The only issue I encountered was being too bold and creating the folder on my Debian LXC ahead of time. This wouldn't let Syncthing create the folder due to the wrong permissions.</p> <p>However, the parent folder needs to provide read+write access for the user if you are running Syncthing as a user service and not a system service.</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#automating-obsidian-with-n8n-after-sync","title":"Automating Obsidian with n8n After Sync","text":"<p>Now, if your obsidian vault is correctly synced to your Debian LXC, and you happen to also be running n8n on that LXC, then you've got a strong case of building some wicked automation sequences to write files to your obsidian vault.</p> <p>In my case, I wanted to automate my Job Hunt tracking. More On this in another post. For now just relish in the fact you have Syncthing setup, working, and can build whatever you would like with n8n to automate things in your vault!</p>"},{"location":"2025/09/04/sync-obsidian-vault-between-windows-11-and-linux-debian-with-syncthing-for-n8n-automations/#conclusion","title":"Conclusion","text":"<p>By combining Syncthing with a Windows 11 workstation and a Debian LXC running on Proxmox, I now have a seamless way to keep my Obsidian vaults perfectly in sync across platforms. More importantly, this setup gave me the foundation to build powerful n8n automations that process and extend my notes without ever leaving my local environment. No cloud lock\u2011in, no subscription fees \u2014 just full control over my data and workflows.</p> <p>If you\u2019ve been looking for a reliable way to sync your Obsidian vaults between Windows and Linux, I highly recommend giving Syncthing a try. Once you have syncing in place, the possibilities are endless when paired with n8n workflows. From automated task management to smart daily note summaries and integrations with your favorite tools.</p>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/","title":"Setting Up k3s on Linux: Quick Tips to User-Friendly Kubernetes","text":"<p>Lightweight K8s distributions have been a game changer for approaching container orchestration in development environments. k3s, Rancher's minimal Kubernetes distribution, stands out for its simplicity and resource efficient packaging. The installation process is remarkably straighforward. However, configuring it for seamless multi-user development requires some additional considerations.</p> <p>In this post, I'll walk you through deploying k3s on a Fedora Linux VM and configuring it for non-root user access.</p>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#why-k3s-and-why-fedora-linux","title":"Why k3s, and Why Fedora Linux?","text":"<p>k3s strips away many complexities found in full Kubernetes distributions but keeps a great API compatibility. Fedora Linux is mostly for practice in a RHEL-like environment, and it includes a modern package management system (dnf and RPM based) as well as systemd integration. Other Linux distributions should work as well.</p> <p>Overall, k3s + Fedora Linux is an ideal platform for:</p> <ul> <li>local development and homelab environments</li> <li>CI/CD pipeline testing and experimentation</li> <li>Kubernetes learning and experimentation</li> <li>Resource-constrained deployments</li> </ul>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#the-easy-part-installation","title":"The Easy Part: Installation","text":"<p>To install k3s, follow the most simple process true to it's name. A single command handles everything:</p> <pre><code>sudo curl -sFl https://get.k3s.io | sh -\n</code></pre> <p>You can always provide options to the above command for more advanced installations</p> <p>Out of the box, this script will automatically:</p> <ul> <li>Download and install the k3s binary</li> <li>Create the systemd service for cluster management</li> <li>Install <code>kubectl</code> for cluster interaction</li> <li>Generate the kubeconfig file at <code>/etc/rancher/k3s/k3s.yaml</code></li> <li>Start the k3s systemd service</li> </ul>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#next-step-verify-your-cluster","title":"Next Step: Verify Your Cluster","text":"<p>After installation it's always a good idea to confirm the cluser is running properly:</p> <pre><code># Check the k3s systemd service status\nsystemctl status k3s.service\n\n# Verify cluster and node status\nsudo kubectl get nodes\nsudo kubectl cluster-info\n</code></pre> <p>You'll know it's running properly when you see a \"Ready\" state for the cluster.</p>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#the-kubeconfig-challenge","title":"The kubeconfig Challenge","text":"<p>Here is where real-world usage begins, and most tutorials will skip over. The default k3s installation creates several challenges for development workflows:</p> <p>Root Ownership Issues</p> <p>The kubeconfig file at <code>/etc/rancher/k3s/k3s.yaml</code> is owned by root with restrictive permissions, which makes it inaccessible to regular user accounts.</p> <p>Localhost Binding</p> <p>The generated kubeconfig uses <code>127.0.0.1</code> as the API server address, which works for root-level access but creates problems when:</p> <ul> <li>Accessing the cluster from external tools</li> <li>Running <code>kubectl</code> from non-root user contexts</li> <li>Integrating with development CI/CD systems</li> </ul> <p>*It's the 2nd one I'm focusing on specifically for this post</p>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#the-user-friendly-access-config","title":"The User-Friendly Access Config","text":"<p>We need to make the kubeconfig accessible to regular users and properly configure for network access. This creates the seamless development experience we are after.</p> <p>Step 1: Create a User-Scoped kubeconfig:</p> <pre><code># Create the standard kubectl configuration directory\nmkdir -p ~/.kube\n\n# Copy the k3s kubeconfig to your user directory\nsudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n\n# Update ownership to your user account\nsudo chown $(id -u):$(id -g) ~/.kube/config\n</code></pre> <p>Step 2: Update IP Configuration:</p> <pre><code># Replacing 127.0.0.1 with your VM's IP address\nsed -i 's/127.0.0.1/&lt;your-vm-ip&gt;/' ~/.kube/config\n</code></pre> <p>*You can find your VM's IP using <code>ip a</code></p> <p>Step 3: Configure Environment Variables:</p> <p>Set the <code>KUBECONFIG</code> environment variable to point to the new user-specific configuration.</p> <pre><code># Set for current session\nexport KUBECONFIG=~/.kube/config\n\n# Make permanent by adding to your shell profile\necho 'export KUBECONFIG=~/.kube/config' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>Step 4: Verify User Access:</p> <p>Run the following as your regular, non-root user:</p> <pre><code>kubectl get nodes\nkubectl cluster-info\n</code></pre>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#troubleshooting-common-issues","title":"Troubleshooting Common Issues","text":"<p>If you encounter permission errors for your user-specified kubeconfig at <code>~/.kube/config</code> then check file ownership and permissions:</p> <pre><code>ls -la ~/.kube/config\n# Should show your username as owner with read/write permissions\n</code></pre>"},{"location":"2025/09/09/setting-up-k3s-on-linux---quick-tips-to-user-friendly-kubernetes/#wrapping-everything-up","title":"Wrapping Everything Up","text":"<p>The k3s installation is remarkably simple, but creating a development environment requires thoughtful configuration. Make sure to properly setup user-scoped kubeconfig files and network accessibility. That's how you transform a basic k3s installation into a powerful, user-friendly Kubernetes development and experimentation platform.</p> <p>This approach scales well from solo development work to team environments, providing a solid foundation for k8s experimentation and development without the overhead of full-scale cluster deployments.</p>"},{"location":"2025/09/23/from-proxmox-to-kubernetes---evolving-my-homelab-part-1/","title":"From Proxmox to Kubernetes - Evolving My Homelab (part 1)","text":""},{"location":"2025/09/23/from-proxmox-to-kubernetes---evolving-my-homelab-part-1/#introduction-current-setup-overview","title":"Introduction &amp; Current Setup Overview","text":"<p>So I've been tinkering with Kubernetes in my homelab for some time now. It's been more of a fun experiment, however, things really started to click for me with how much I enjoyed the declarative orchestration posibilities. Kubernetes os known for container orchestration, and it does allow both imperative and declarative management.</p> <p>Well, I've already been doing imperative management across my whole homelab with my Proxmox setups, VMs, LXCs, and containers within VMs. So I knew what that required. It's a great way to learn, and truly helps with building a strong problem solving mentality because you are making the configuration update and see the immediate results of your change. So when things break, you can just review what you had just done and learn why it happened and how to resolve it.</p> <p>But I digress, my current homelab architecture is two Proxmox hosts, with most self-hosted apps/services running in LXCs. Things like Docmost, Caddy, n8n, and PiHole. Then I've got a few VMs to host my Ansible workflows for configuration management, and my Omada SDN which is running in a Podman container. There is also OpenMediaVault for shared storage and backups, as well as Proxmox Backup Server. This setup gets the job done for me, but it all requires very hands-on configuration. Could I just script everything and use Ansible to fill in my orchestration gaps? Sure. But if a service/app has a problem, it's just done. It won't automatically come back up unless I have that process also configured.</p> <p>That's where this whole Kubernetes idea came to light. I'd never considered it previously for orchestrating all my self-hosted apps/services. And it wasn't until I had a better grasp on the theory behind GitOps that it all started to really make sense.</p>"},{"location":"2025/09/23/from-proxmox-to-kubernetes---evolving-my-homelab-part-1/#what-this-evolution-means-for-my-homelab","title":"What this evolution means for my homelab","text":"<p>I'm already in the process of transitioning my homelab architecture to utilizing Kubernetes. Here's what it will look like:</p> <ol> <li> <p>Still relying on Proxmox \"under the hood\" so to speak.</p> <p>I want to still have challenges. Rather than just go with 3 separate devices that run Talos, I'm going to have Kubernetes nodes running as VM's within my Proxmox. Makes me remember that networking, firewalls, and connectivity are super important, and great knowledge to keep up with.</p> </li> <li> <p>Transition all my self-hosted apps to configuration files on GitHub</p> </li> </ol> <p>I've created a new repo on my GitHub titled \"homelab\" where I plan to build a complete GitOps process, complete with CI/CD, and automations. I'm going to utilized FluxCD in my Kubernetes cluster for this.</p> <p>This will allow for me to declaratively control all of these through each configuration file, and through Pull Requests. More akin to a DevOps lifecycle, which is something I still strive to get more involved with in my career.</p> <ol> <li>Continue to use Ansible for configuration management outside of K8s</li> </ol> <p>I still want to use Ansible for automating and orchestrating the VM configurations for Kubernetes Nodes, as well as infrastructure backup configurations.</p> <p>Eventually I'll add in Terraform for true Infrastructure as Code (IaC) workflows and fully manage my Proxmox and VMs this way. But that's for another post.</p> <p>Stay tuned as I write more about this evolution of my homelab. I plan to release several parts that detail the journey.</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/","title":"Creating Linkding Deployment with FluxCD","text":"<p>Ever since I started tinkering with FluxCD in my homelab Kubernetes cluster, I've been on a kick with maximizing both best practices with it, and also automating the orchestration of my self-hosted services.</p> <p>It's quite a ride! But as this post is my process for getting the Linkding bookmark service deployed, I will not be going into detail on how to get FluxCD setup and configured. That's a prerequisite you have hopefully already gone through.</p> <p>For a full breakdown of this structure and the files you can check out my homelab GitHub repository at https://github.com/cyberwatchdoug/homelab/tree/main</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#architecture","title":"Architecture","text":"<p>My FluxCD is organized by folders for my apps labeled apps/base and apps/staging where Staging is currently my testing environment, while the Base holds all the standard templates that work in either a testing environment or eventually a production environment. This primarily saves me from duplicating yaml configuration files, and allows me to reuse the base configurations or adjust them as necessary for the given environment.</p> <p>Here'a visual of my folder setup:</p> <pre><code>homelab\n\u2514\u2500\u2500apps\n   \u251c\u2500\u2500 base\n   \u2502\u00a0\u00a0 \u2514\u2500\u2500 linkding\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 deployment.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 namespace.yaml\n   \u2502\u00a0\u00a0     \u2514\u2500\u2500 storage.yaml\n   \u2514\u2500\u2500 staging\n       \u2514\u2500\u2500 linkding\n           \u2514\u2500\u2500 kustomization.yaml\n</code></pre>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#staging-files","title":"Staging Files","text":"<p>Currently, I only have the one <code>kustomization.yaml</code> file in my staging directory. This points to my apps/base/linkding directory, and informs Flux to then read the <code>kustomization.yaml</code> file in that directory.</p> <pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: linkding\nresources:\n  - ../../base/linkding/\n</code></pre>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#base-files","title":"Base files","text":"<p>The base files are where the magic happens. Again, I have created a <code>kustomization.yaml</code> file, which then lists out each of the other YAML files in this directory, informing Flux to read and apply them:</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#kustomizationyaml","title":"kustomization.yaml","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: linkding\nresources:\n  - namespace.yaml\n  - storage.yaml\n  - deployment.yaml\n</code></pre> <p>Going down that list, the next file is <code>namespace.yaml</code> where I create the namespace I will be using to group my linkding deployment and related resources:</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#namespaceyaml","title":"namespace.yaml","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: linkding\n</code></pre> <p>Up next is <code>storage.yaml</code> which creates the PersistentVolumeClaim (PVC) I will be using for linkding's <code>/etc/linkding/data</code> directory. This is where the linkding service stores all of its configuration files and most critcally the database file for user accounts. I'm using a PVC so that when this Deployment get's recreated the data will persist.</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#storageyaml","title":"storage.yaml","text":"<pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: linkding-data-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre> <p>Next is <code>deployment.yaml</code>. This file describes the linkding Deployment and associated configurations I want for the Pod and Containers inside it.</p> <p>A note on security: I've ensured that the user, group, and filesystem are all specified as the linkding user, known as Unprivileged. Without those changes, the Container would be running as root, which is known as Privileged and we don't want our Containers running as Privileged.</p>"},{"location":"2025/09/24/creating-linkding-deployment-with-fluxcd/#deploymentyaml","title":"deployment.yaml","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: linkding\n  name: linkding\n  namespace: linkding\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkding\n  template:\n    metadata:\n      labels:\n        app: linkding\n    spec:\n      securityContext:\n        runAsUser: 33   # www-data user ID\n        runAsGroup: 33  # www-data group ID\n        fsGroup: 33     # www-data group ID\n      volumes:\n        - name: linkding-data\n          persistentVolumeClaim:\n            claimName: linkding-data-pvc\n      containers:\n        - name: linkding\n          image: sissbruecker/linkding:1.41.0\n          ports:\n            - containerPort: 9090\n          securityContext:\n            allowPrivilegeEscalation: false\n          volumeMounts:\n            - name: linkding-data\n              mountPath: \"/etc/linkding/data\"\n</code></pre> <p>With everything in place, deploying Linkding with FluxCD becomes a seamless, repeatable process that fits into my homelab workflow. Not only does this approach keep my configurations tidy, but it also gives me the flexibility to tweak, upgrade, and scale as my needs evolve. If you're looking to bring order and automation to your self-hosted services, I hope showing you my process for deploying Linkding helps get you started or inspires you to refine your own setup.</p> <p>Happy homelabing folks!</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/","title":"Secrets Management With External Secrets Operator and 1Password (part 1)","text":"<p>In this first part for my Secrets Management with External Secrets Operator (ESO) and 1Password series, I'm going to detail how to get ESO deployed through GitOps using Flux, Kustomization resources, and Helm resources. All of these configuration files can be found in my homelab GitHub repository located here: https://github.com/cyberwatchdoug/homelab/tree/main</p> <p>What exactly is External Secrets Operator, and why should we use it? Great question. ESO is a Kubernetes operator that solves the dilemma of secrets management in Kubernetes from external sources. The list of providers is lengthy, but it includes important players like AWS, Google, Azure, HashiCorp, CyberArk, and 1Password. The goal of this operator is to synchronize secrets from these external sources into Kubernetes secrets, so they can be more easily accessed and used throughout the cluster.</p> <p>First, let's take a look at the structure for this GitOps repository. In this post I'll detail most of these, with some detailed in my part 2 found here.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#file-and-folder-structure","title":"File and Folder Structure","text":"<pre><code>infrastructure\n\u251c\u2500\u2500\u2500base\n\u2502   \u2514\u2500\u2500\u2500external-secrets\n\u2502       \u2502   deployment-crds.yaml\n\u2502       \u2502   deployment-crs.yaml\n\u2502       \u2502   kustomization.yaml\n\u2502       \u2502   namespace.yaml\n\u2502       \u2502   release.yaml\n\u2502       \u2502   repositories.yaml\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500crs\n\u2502               cluster-secret-store.yaml\n\u2502               kustomization.yaml\n\u2502\n\u2514\u2500\u2500\u2500staging\n    \u2514\u2500\u2500\u2500external-secrets\n            kustomization.yaml\n</code></pre>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#deploying-external-secrets-operator","title":"Deploying External Secrets Operator","text":""},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#kustomization-files-everywhere","title":"Kustomization Files Everywhere","text":"<p>Like most things in my homelab, it all starts with a Kustomization YAML file. In this case, I have a file <code>infrastructure.yaml</code> located in the <code>clusters/staging/</code> directory, which is the same place where the <code>apps.yaml</code> Kustomization file lives. The sole purpose for this file is a Flux Kustomization resource which tells Flux to continuously apply and manage all Kubernetes manifests found in the <code>path:</code> field. In my case, it points to <code>infrastructure/staging/</code>.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#infrastructureyaml","title":"infrastructure.yaml","text":"<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: infrastructure\n  namespace: flux-system\nspec:\n  interval: 1h\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: \"./infrastructure/staging\"\n  prune: true\n  timeout: 1m\n  retryInterval: 1m\n</code></pre> <p>Up next is, you guessed it, another Kustomization YAML file! There is currently only one operator folder in the infrastructure directory: <code>external-secrets</code>. More to come later, but since this post is specific to ESO, it's all we need to discuss.</p> <p>There is also only one file in the <code>infrastructure/staging/external-secrets/</code> folder: the Kustomization YAML file. But this one is different from the one above. Can you spot the main difference? In the apiVersion, the one above is based on the Kustomize spec from FluxCD, while this one here is based on the Kustomize spec from Kubernetes. This can be confusing at first, because both FluxCD and Kubernetes have a <code>Kustomization</code> kind, even though they are technically completely different objects.</p> <p>The important thing to know here is that the apiVersion provided will decide which Kubernetes controller gets the hand off.</p> <p>The one detailed below (the K8s Kustomization) is for the native Kustomize API group and is used for templating and to organize our YAML manifests with patches (future topic). The only declaration inside is the resources section, which defines where the template needs to look for additional Kustomization files. In this case, it directs to the <code>../../base/external-secrets/</code> folder, which is two levels up (the <code>../../</code> bit).</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#kustomizationyaml-infrastructurestagingexternal-secrets","title":"kustomization.yaml <code>infrastructure/staging/external-secrets</code>","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - ../../base/external-secrets/\n</code></pre> <p>And the last Kustomization YAML file for this section is another K8s Kustomize object reference. Like the previous one, it declares resources the template needs to go find and apply to the cluster.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#kustomizationyaml-infrastructurebaseexternal-secrets","title":"kustomization.yaml <code>infrastructure/base/external-secrets/</code>","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - namespace.yaml\n  - repositories.yaml\n  - deployment-crds.yaml\n  - release.yaml\n  - deployment-crs.yaml\n</code></pre>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#namespace-setup","title":"Namespace Setup","text":"<p>First in the list is our <code>namespace.yaml</code> which does just that: creates the namespace for ESO named external-secrets. In Kubernetes, a namespace is the mechanism for isolating groups of resources. In our case, we are isolating all these external-secrets resources together.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#namespaceyaml","title":"namespace.yaml","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-secrets\n</code></pre>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#helm-setup","title":"Helm Setup","text":"<p>To get started using Helm, you'll first need to make sure your Kubernetes cluster has it installed. In my case, k3s comes with it included. You can check if your Kubernetes cluster has Helm by running the command <code>helm version</code>.</p> <p>For my homelab, I'm configuring the Helm Repository using a FluxCD object <code>HelmRepository</code> that provides the url specific for ESO Helm charts. Because it's a FluxCD object, the <code>HelmRepository</code> source will be managed by Flux.</p> <p>To list all Helm repos managed by Flux, use this command: <code>flux get sources helm --all-namespace</code>.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#repositoriesyaml","title":"repositories.yaml","text":"<pre><code>apiVersion: source.toolkit.fluxcd.io/v1\nkind: HelmRepository\nmetadata:\n  name: external-secrets\n  namespace: flux-system\nspec:\n  interval: 10m\n  url: https://charts.external-secrets.io\n---\napiVersion: source.toolkit.fluxcd.io/v1\nkind: GitRepository\nmetadata:\n  name: external-secrets\n  namespace: flux-system\nspec:\n  interval: 10m\n  ref:\n    tag: v0.20.0\n  url: https://github.com/external-secrets/external-secrets\n</code></pre> <p>Now that we have declared the Helm repo that we want Flux to manage, we need another file: <code>release.yaml</code>. This defines a Flux <code>HelmRelease</code> resource object for deploying ESO into the Kubernetes cluster (using Helm for deployment in the end).</p> <p>You'll see that we reference the <code>HelmRepository</code> created in the previous YAML file. We are also setting <code>installCRDs: false</code> to prevent a special race condition where the Custom Resources (CRs), namely <code>SecretStore</code>, and <code>ClusterSecretStore</code>, could be deployed before the Custom Resource Definitions (CRDs) which are needed to recognize them (the CRs). This will be explained in more detail in part 2.</p>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#releaseyaml","title":"release.yaml","text":"<pre><code>apiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: external-secrets\n  namespace: flux-system\nspec:\n  # Override release name to avoid pattern Namespace-Release\n  releaseName: external-secrets\n  targetNamespace: external-secrets\n  interval: 10m\n  chart:\n    spec:\n      chart: external-secrets\n      version: 0.20.1\n      sourceRef:\n        kind: HelmRepository\n        name: external-secrets\n        namespace: flux-system\n  values:\n    installCRDs: false\n  install:\n    createNamespace: true\n</code></pre>"},{"location":"2025/09/25/secrets-management-with-external-secrets-operator-and-1password-part-1/#wrap-up","title":"Wrap-Up","text":"<p>With the foundational setup for External Secrets Operator in place, my homelab now has a GitOps-driven approach to deploying and managing ESO in my Kubernetes cluster using Flux and Helm. In part 2, I'll dive deeper into configuring ESO to actually sync secrets from 1Password, including the setup of ClusterSecretStores and the necessary Custom Resources. I'll also explain in more detail the race condition I alluded to. All of this to ensure you can securely manage secrets in Kubernetes clusters end-to-end.</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/","title":"Updated Linkding Deployment with FluxCD","text":"<p>This is an update to my previous post regarding my process for creating a Linkding self-hosted service with FluxCD in my homelab Kubernetes cluster.</p> <p>You can read the original post here: Creating Linkding Deployment with FluxCD</p> <p>After getting my Linkding deployment working through my FluxCD GitOps lifecycle, I quickly realized that I was missing some key functionality and configuration steps.</p> <p>The first one being that my Linkding app wasn't being exposed for me to access locally on my network. It was only accessible from a node in my cluster that could access the cluster IP addresses. This a is problem if I'm planning to use Linkding for all my bookmarks!</p> <p>The next one being that the Deployment does not declare any superuser account. In the original version of my Deployment I was required to perform an <code>exec</code> command inside the Container to create my superuser name and password before I could ever login. This was using a python script and very tedious! Not what I want if my aim is to have a declarative, stateful Deployment where we could potentially deploy Linkding to a brand new Kubernetes cluster with a superuser already setup and configured. I have the PersistentVolumeClaim setup for the <code>data</code> directory to persist within the cluster, but an initial or bootstrap deploy to a brand new cluster would not result in any superuser account getting setup. This relates to the idea of idempotentency, where I want the Deployment to be applied the first time and any number of times after that without changing the outcome beyond the initial deployment.</p> <p>These updates support declarative, repeatable deployments of linkding and improves security by not hardcoding credentials.</p> <p>For a full breakdown of this updated structure to my Linkding Deployment you can check out my homelab GitHub repository at https://github.com/cyberwatchdoug/homelab/tree/main</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#updated-architecture","title":"Updated Architecture","text":"<p>If you recall, my GitOps using Flux is organized by folders for my apps labeled apps/base and apps/staging. The updates to that architecture is the addition of three files <code>service.yaml</code>, <code>ingress.yaml</code>, and <code>secrets.yaml</code> as well as updating the <code>kustomization.yaml</code> file in the apps/base/linkding/ directory to add these two new files to the resources list.</p> <p>Here's a visual of my updated folder setup:</p> <pre><code>homelab\n\u2514\u2500\u2500apps\n   \u251c\u2500\u2500 base\n   \u2502\u00a0\u00a0 \u2514\u2500\u2500 linkding\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 deployment.yaml (updated)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml (updated)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 namespace.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 storage.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 service.yaml (new)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 ingress.yaml (new)\n   \u2502\u00a0\u00a0     \u2514\u2500\u2500 secrets.yaml (new)\n   \u2514\u2500\u2500 staging\n       \u2514\u2500\u2500 linkding\n           \u2514\u2500\u2500 kustomization.yaml\n</code></pre>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#updated-kustomization-file","title":"Updated Kustomization File","text":"<p>Nothing major here, just the addition of <code>service.yaml</code>, <code>ingress.yaml</code> and <code>secrets.yaml</code> files to the resources list:</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#kustomizationyaml-appsbaselinkding","title":"kustomization.yaml <code>apps/base/linkding/</code>","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: linkding\nresources:\n  - namespace.yaml\n  - storage.yaml\n  - deployment.yaml\n  - service.yaml \n  - secrets.yaml\n</code></pre>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#new-serviceyaml-file","title":"New <code>service.yaml</code> File","text":"<p>This file defines a Kubernetes Service resource for my Linkding app. It will expose the app internally within the k8s cluster on port 9090, forwarding traffic to the application's pods on target port 9090. I'm using a selector of <code>app: linkding</code> to ensure traffic routing is accurate. Using <code>type: ClusterIP</code> means it is only accessible within the cluster. This will make sense after I explain the Ingress resource created in the next file.</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#serviceyaml","title":"service.yaml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: linkding\n  namespace: linkding\nspec:\n  selector:\n    app: linkding\n  ports:\n    - port: 9090\n      targetPort: 9090\n  type: ClusterIP\n</code></pre>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#new-ingressyaml-file","title":"New <code>ingress.yaml</code> File","text":"<p>This file defines a Kubernetes Ingress resource for my Linkding app. It will enable external HTTP access to the service (defined above). Since k3s uses Traefik as the ingress controller, this will route requests to <code>linkding.local</code> to the internal <code>linkding</code> Service on port 9090. I've specified <code>path: /</code> to make sure all HTTP requests to <code>linkding.local</code> get forwarded to the <code>linkding</code> Service. In short, this allows me to access my Linkding app locally on my network by the specified hostname.</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#ingressyaml","title":"ingress.yaml","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: linkding\n  namespace: linkding\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  entryPoints:\n    - web # use 'web' for HTTP only\n  rules:\n    - host: linkding.local\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: linkding\n              port:\n                number: 9090\n</code></pre> <p>Something I'd like to point out here regarding exposing multiple apps that all listen on the same container port. Inside Kubernetes, there is no port conflict because each Service is an isolated abstraction with its own virtual IP inside the cluster. But for it to work that way the following must be true:</p> <ol> <li>Each Service has a unique name.</li> <li>Each Ingress path or hostname is unique.</li> <li>Each Service is of type <code>ClusterIP</code>.</li> </ol>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#new-secretsyaml-file","title":"New <code>secrets.yaml</code> File","text":"<p>For this resource, you must have already deployed and configured the External Secrets Operator in your cluster. I've detailed how I have done this in the following posts:</p> <ul> <li>Secrets Management With External Secrets Operator and 1Password (part 1)</li> <li>Secrets Management With External Secrets Operator and 1Password (part 2)</li> </ul> <p>The file below defines an ExternalSecret resource for my Linkding app. It essentially instructs the External Secrets Operator to retrieve the secret keys LD_SUPERUSER_NAME and LD_SUPERUSER_PASSWORD from an external provider and then store them in a cluster Secret resource named linkding-container-env. The external provider is referenced in the <code>secretStoreRef</code> section. I've set this to refresh every 12 hours, but will likely change that to a shorter interval in the future.</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#secretsyaml","title":"secrets.yaml","text":"<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: linkding-container-env\n  namespace: linkding\nspec:\n  refreshInterval: 12h\n  secretStoreRef:\n    name: onep-store\n    kind: ClusterSecretStore\n  data:\n    - secretKey: LD_SUPERUSER_NAME\n      remoteRef:\n        key: \"Linkding/username\"\n    - secretKey: LD_SUPERUSER_PASSWORD\n      remoteRef:\n        key: \"Linkding/password\"\n</code></pre>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#updated-deploymentyaml-file","title":"Updated <code>deployment.yaml</code> File","text":"<p>The additions to this file enable the use of the secrets retrieved from the external provider configured in the previous file (<code>secrets.yaml</code>). </p> <p>To clarify, the Secret resource named linkding-container-env will contain two secrets: LD_SUPERUSER_NAME and LD_SUPERUSER_PASSWORD, with their respective values retrieved from the external provider.</p> <p>The new <code>envFrom:</code> block takes the key=value pairs from the referenced Secret resource and passes them as environment variables to the linkding container. This is why the secret keys match the environment variable options for the Linkding application.</p> <p>There is no need to worry about these variables being set every time a new linkding container is deployed. They will not overwrite an existing superuser if one already exist. These variables are only used when the database is empty, specifically during any initial startup. If there is an existing database, they are ignored.</p>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#deploymentyaml-updated-containers-section","title":"deployment.yaml (updated <code>containers</code> section)","text":"<pre><code>      containers:\n        - name: linkding\n          image: sissbruecker/linkding:1.41.0\n          ports:\n            - containerPort: 9090\n          securityContext:\n            allowPrivilegeEscalation: false\n          envFrom:\n            - secretRef:\n                name: linkding-container-env\n          volumeMounts:\n            - name: linkding-data\n              mountPath: \"/etc/linkding/data\"\n</code></pre>"},{"location":"2025/09/26/updated-linkding-deployment-with-fluxcd/#wrap-up","title":"Wrap-Up","text":"<p>With these improvement, my Linkding deployment is now fully declarative, secure, and accessible on my local network. By integrating FluxCD, Kustomize, and the External Secrets Operator, I have streamlined the setup and ongoing management of my self-hosted service. This approach will also ensure that deployments are repeatable, credentials are managed securely, and the application is always available as intended.</p> <p>If you have questions or want to share your own experiences with GitOps and Kubernetes, feel free to reach out. Find me <code>@cyberwatchdoug</code> on most places.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/","title":"Secrets Management With External Secrets Operator and 1Password (part 2)","text":"<p>In this second part for my Secrets Management with External Secrets Operator (ESO) and 1Password, I will be detailing how I configured my ESO deployment through GitOps using Flux, Kustomization, and Secrets resources. You can read the first part here: Secrets Management With External Secrets Operator and 1Password (part 1).</p> <p>A recap on why ESO: the goal of the ESO operator is to synchronize secrets from these external sources into Kubernetes secrets, so they can be more easily accessed and used throughout the cluster.</p> <p>All of these configuration files can be found in my homelab GitHub repository located here: https://github.com/cyberwatchdoug/homelab/tree/main</p> <p>First let's recap and take another look at the structure of the GitOps repository. I will be focusing on describing the remaining files in this post, as well as how they work together with those previously detailed in part 1 found here which together deploy a declarative configuration for secrets management in my homelab using GitOps.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#file-and-folder-structure","title":"File and Folder Structure","text":"<pre><code>infrastructure\n\u251c\u2500\u2500\u2500base\n\u2502   \u2514\u2500\u2500\u2500external-secrets\n\u2502       \u2502   deployment-crds.yaml\n\u2502       \u2502   deployment-crs.yaml\n\u2502       \u2502   kustomization.yaml\n\u2502       \u2502   namespace.yaml\n\u2502       \u2502   release.yaml\n\u2502       \u2502   repositories.yaml\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500crs\n\u2502               cluster-secret-store.yaml\n\u2502               kustomization.yaml\n\u2502\n\u2514\u2500\u2500\u2500staging\n    \u2514\u2500\u2500\u2500external-secrets\n            kustomization.yaml\n</code></pre> <p>In this part 2, we are concerned with the files in the <code>infrastructure/base/external-secrets/crs/</code> folder as well as the <code>deployment-crds.yaml</code> and <code>deployment-crs.yaml</code> files.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#secrets-management-for-eso","title":"Secrets Management for ESO","text":"<p>In part 1, I did not make mention of credentials that would be needed in order to use ESO to access the external provider. As best practice, we do not want this token hard coded into our infrastructure manifests. The simplest is storing the credentials needed inside a Kubernetes Secret store, which can be manually created, or fully automated as part of any infrastructure or configuration automations when setting up your K8s nodes. I won't cover this in detail here but will likely have a future post with more information.</p> <p>I'll point out where the Secret store is referenced in the file below.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#deploying-custom-resource-definitions-crds","title":"Deploying Custom Resource Definitions (CRDs)","text":"<p>As pointed out in part 1, there is a race condition we are trying to have control over when Custom Resources (CRs) are created compared to the Custom Resource Definitions (CRDs). The problem is that we have two controllers working to deploy ESO. One is the HelmController, and the other is the KustomizeController. As both controllers manage resources independently, there is no control over making one wait for the other. What is needed is a process to control the CRDs being deployed before the CRs.</p> <p>The first step in controlling this process was shown in part 1, where the <code>deployment.yaml</code> file instructed the HelmController not to install CRDs. So the next step is naturally to create the Kustomization file to deploy the CRDs, which is the <code>deployment-crds.yaml</code> file listed below.</p> <p>The <code>deployment-crds.yaml</code> file defines a Flux Kustomization resource in the flux-system namespace. It instructs Flux to apply manifests found in the <code>/deploy/crds</code> directory of the referenced GitRepository we created in the <code>repositories.yaml</code> file. In this case, it points to the GitHub repo for ESO itself. There is a lone file in that directory called bundle.yaml. This file, at well over 27,000 lines, deploys all the CRDs required for the CRs.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#deployment-crdsyaml","title":"deployment-crds.yaml","text":"<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: external-secrets-crds\n  namespace: flux-system\nspec:\n  interval: 10m\n  path: ./deploy/crds\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: external-secrets\n</code></pre>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#deploying-custom-resources-crs","title":"Deploying Custom Resources (CRs)","text":"<p>For the <code>deployment-crs.yaml</code> we define another Flux Kustomization resource in the flux-system namespace. This file instructs Flux to apply resources found in the <code>./infrastructure/base/external/crs/</code> directory. But the magic is that it will only do this after ensuring the <code>external-secrets-crs</code> dependency is present. The source referenced is the flux-system GitRepository, which is the same repo that FluxCD watches to synchronize the cluster. So a reference to itself!</p> <p>So the last step, detailed in the next section, is to place all the desired CRs to be deployed, for example the <code>cluster-secret-store.yaml</code> for reaching the 1Password SDK.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#deployment-crsyaml","title":"deployment-crs.yaml","text":"<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: external-secrets-crs\n  namespace: flux-system\nspec:\n  dependsOn:\n    - name: external-secrets-crds\n  interval: 10m\n  path: ./infrastructure/base/external/crs\n  prune: true\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n</code></pre>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#crs-to-deploy","title":"CRs to Deploy","text":"<p>As pointed out, within the <code>./infrastructure/base/external/crs</code> directory are all the CRs to be deployed in our cluster. For my cluster, this is my lone manifest with details on connecting with the 1Password SDK to my vault.</p> <p>This file does a few things. First it's of kind <code>ClusterSecretStore</code> meaning it is available cluster-wide. To restrict it you can also use <code>SecretStore</code> to bind it to a single namespace. Next it declares the provider, onepasswordSDK as well as the vault name to be accessed. And finally it declares the secret reference to be used for authentication to the 1Password SDK.</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#cluster-secret-storeyaml","title":"cluster-secret-store.yaml","text":"<pre><code>apiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: onep-store\nspec:\n  provider:\n    onepasswordSDK:\n      vault: Homelab\n      auth:\n        serviceAccountSecretRef:\n          name: onep-creds\n          key: OnePToken\n          namespace: external-secrets\n</code></pre> <p>Finally, a Kustomization file in the <code>crs</code> directory ensures FluxCD deploys all manifests within the folder:</p>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#kustomizationyaml-infrastructurebaseexternal-secretscrs","title":"kustomization.yaml (<code>./infrastructure/base/external-secrets/crs</code>)","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - cluster-secret-store.yaml\n</code></pre>"},{"location":"2025/09/27/secrets-management-with-external-secrets-operator-and-1password-part-2/#wrap-up","title":"Wrap-Up","text":"<p>To wrap things up: using FluxCD to orchestrate the deployment of ESO and its associated resources ensures a reliable and repeatable process for secrets management in Kubernetes. By separating the deployment of CRDs and CRs and leveraging GitOps principles, you gain fine-grained control over resource application order and configuration drift. This approach not only enhances security by keeping sensitive credentials out of your manifests, but also streamlines operations and makes your homelab infrastructure more robust and maintainable. Stay tuned for future posts where I\u2019ll dive deeper into automating secret creation and integrating additional providers!</p>"},{"location":"2025/09/29/from-proxmox-to-kubernetes---evolving-my-homelab-part-2/","title":"From Proxmox to Kubernetes - Evolving My Homelab (part 2)","text":"<p>This is part 2 of my series detailing a transition of my Homelab architecture to using Kubernetes with Proxmox. You can check out part one here: From Proxmox to Kubernetes - Evolving My Homelab (part 1)</p> <p>In this post, I continue my journey evolving my homelab from a simple Proxmox setup to a more robust Kubernetes-based architecture. Building on part one, I\u2019ll share what worked, what didn\u2019t, and how my approach to self-hosting and automation has changed over time.</p>"},{"location":"2025/09/29/from-proxmox-to-kubernetes---evolving-my-homelab-part-2/#my-proxmox-foundation-what-worked-and-what-didnt","title":"My Proxmox Foundation: What Worked (And What Didn't)","text":"<p>Starting out, I had a single device with Proxmox installed and configured. This was as an experiment but also with the intent to get a proper firewall configured to protect my home network. For this, I ran pfSense as a VM within Proxmox, and then configured networking to use pfSense completely. My device had two network interfaces, so it was rather simple to configure. One interface was for my incoming network traffic, and the second interface for my home network. For anyone wondering, I only allowed the management of Proxmox to occur on my home network interface.</p> <p>Eventually I added a PCI card with four additional network interfaces. This was mostly because I needed them for other home network connections and I wanted to experiment configuring pfSense for this rather than just buying a simple unmanaged switch.</p> <p>What worked with my Proxmox was having an amazing learning lab at home to get my hands dirty with hypervisor configuration, VMs, and all the glory that is resource usage. Using pfSense allowed me to also dig deep into firewall rules, VLANs, and segmenting network traffic in more ways than just subnetting.</p> <p>What didn't work was obvious after a few short months. Everything was running on a single device. If I had to update the hypervisor or pfSense, the whole network had to lose connectivity. If there was a hardware failure, the whole network had to lose connectivity. You get the idea. What concerned me most was that I was running all this on just old desktop hardware. It may have been vastly overboard in terms of compute (CPU and Memory) but wasn't built specifically for these tasks.</p> <p>Eventually I purchased a Protectli Vault, installed OPNsense, and configured that as the firewall for my home network. My Proxmox instead became my homelab to experiment with self-hosted apps. This became my playground for a few years.</p>"},{"location":"2025/09/29/from-proxmox-to-kubernetes---evolving-my-homelab-part-2/#self-hosted-haven-on-proxmox","title":"Self-Hosted Haven on Proxmox","text":"<p>I first started with running Docker containers in Debian-based linux distributions. This was itself a great experience. Learning containers, how to configure them for persistent storage and network access, and problem-solving all the little things that happened along the way - that was when the real fun in my homelab began. And everything being controlled with code through a <code>docker-compose</code> YAML file.</p> <p>My little self-hosted haven eventually grew to deploying containers directly on Proxmox, known as LXCs. Most of these were again running on Debian-based linux. However, I had started to deploy Fedora VMs (RedHat/RPM-based) and experimenting with Podman. My interest in Podman came about when reading about container privileges, and wanting to run my containers as non-root (unprivileged). That's outside the scope of my story here (maybe a future post). I now had LXC containers for Docmost, PiHole, Caddy, Paperless-ngx, Postiz, and n8n. And on my Fedora VM a Podman container for my Omada SDN, which manages my switch, and access point for my home network.</p> <p>While I was starting to utilize Ansible for configuration management, it was not quite everything I hoped it would be. I began with the idea of \"how can I use Ansible to build all this up from scratch\" and quickly learned the concept of idempotency. So I developed Ansible playbooks to validate initial system/container configurations and continuously ensure configuration integrity over time.</p> <p>This was great, but if a container went down unexpectedly, I had to manually troubleshoot and restore it. I had no way of orchestrating this process with Ansible to the degree I wanted. In my mind, Ansible was not well-suited for managing containerized workloads. It might be excellent at provisioning and enforcing, but lacks the dynamic scheduling, self-healing, and service discovery required for container orchestration.</p> <p>This was a homelab environment after all, where I constantly experiment and tinker. To address these newfound needs, I decided to adopt Kubernetes, which provides native support for many of the limitations I found with Ansible. I'm still using Ansible, and paired with Kubernetes it's been a game changer.</p>"},{"location":"2025/09/29/from-proxmox-to-kubernetes---evolving-my-homelab-part-2/#wrap-up","title":"Wrap-Up","text":"<p>Moving from a single-device Proxmox setup to a more distributed and orchestrated environment will fundamentally improved both the reliability and flexibility of my homelab. By layering Ansible\u2019s configuration management with Kubernetes\u2019 orchestration capabilities, I\u2019ll be able to automate, scale, and recover my self-hosted services far more efficiently than ever before. This evolution should not only solve the limitations I encountered with manual intervention and single points of failure, but also open up new possibilities for experimentation and growth. Stay tuned for part 3, where I\u2019ll dive into the details of my implementation strategy for Kubernetes and how it will further transform my homelab.</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/","title":"Updating Linkding Deployment With Cloudflare Tunnels","text":"<p>This is an update to my previous post regarding my process for creating a Linkding self-hosted service with FluxCD in my homelab Kubernetes cluster.</p> <p>You can read the original post here: Creating Linkding Deployment with FluxCD</p> <p>After getting my Linkding deployment working through my FluxCD GitOps lifecycle, I quickly realized that I was missing some key functionality and configuration steps.</p> <p>The first one being that my Linkding app wasn't being exposed for me to access locally on my network. It was only accessible from a node in my cluster that could access the cluster IP addresses. This a is problem if I'm planning to use Linkding for all my bookmarks!</p> <p>The next one being that the Deployment does not declare any superuser account. In the original version of my Deployment I was required to perform an <code>exec</code> command inside the Container to create my superuser name and password before I could ever login. This was using a python script and very tedious! Not what I want if my aim is to have a declarative, stateful Deployment where we could potentially deploy Linkding to a brand new Kubernetes cluster with a superuser already setup and configured. I have the PersistentVolumeClaim setup for the <code>data</code> directory to persist within the cluster, but an initial or bootstrap deploy to a brand new cluster would not result in any superuser account getting setup. This relates to the idea of idempotentency, where I want the Deployment to be applied the first time and any number of times after that without changing the outcome beyond the initial deployment.</p> <p>These updates support declarative, repeatable deployments of linkding and improves security by not hardcoding credentials.</p> <p>For a full breakdown of this updated structure to my Linkding Deployment you can check out my homelab GitHub repository at https://github.com/cyberwatchdoug/homelab/tree/main</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#updated-architecture","title":"Updated Architecture","text":"<p>If you recall, my GitOps using Flux is organized by folders for my apps labeled apps/base and apps/staging. The updates to that architecture is the addition of three files <code>service.yaml</code>, <code>ingress.yaml</code>, and <code>secrets.yaml</code> as well as updating the <code>kustomization.yaml</code> file in the apps/base/linkding/ directory to add these two new files to the resources list.</p> <p>Here's a visual of my updated folder setup:</p> <pre><code>homelab\n\u2514\u2500\u2500apps\n   \u251c\u2500\u2500 base\n   \u2502\u00a0\u00a0 \u2514\u2500\u2500 linkding\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 deployment.yaml (updated)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 kustomization.yaml (updated)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 namespace.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 storage.yaml\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 service.yaml (new)\n   \u2502\u00a0\u00a0     \u251c\u2500\u2500 ingress.yaml (new)\n   \u2502\u00a0\u00a0     \u2514\u2500\u2500 secrets.yaml (new)\n   \u2514\u2500\u2500 staging\n       \u2514\u2500\u2500 linkding\n           \u2514\u2500\u2500 kustomization.yaml\n</code></pre>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#updated-kustomization-file","title":"Updated Kustomization File","text":"<p>Nothing major here, just the addition of <code>service.yaml</code>, <code>ingress.yaml</code> and <code>secrets.yaml</code> files to the resources list:</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#kustomizationyaml-appsbaselinkding","title":"kustomization.yaml <code>apps/base/linkding/</code>","text":"<pre><code>apiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: linkding\nresources:\n  - namespace.yaml\n  - storage.yaml\n  - deployment.yaml\n  - service.yaml \n  - secrets.yaml\n</code></pre>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#new-serviceyaml-file","title":"New <code>service.yaml</code> File","text":"<p>This file defines a Kubernetes Service resource for my Linkding app. It will expose the app internally within the k8s cluster on port 9090, forwarding traffic to the application's pods on target port 9090. I'm using a selector of <code>app: linkding</code> to ensure traffic routing is accurate. Using <code>type: ClusterIP</code> means it is only accessible within the cluster. This will make sense after I explain the Ingress resource created in the next file.</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#serviceyaml","title":"service.yaml","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: linkding\n  namespace: linkding\nspec:\n  selector:\n    app: linkding\n  ports:\n    - port: 9090\n      targetPort: 9090\n  type: ClusterIP\n</code></pre>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#new-ingressyaml-file","title":"New <code>ingress.yaml</code> File","text":"<p>This file defines a Kubernetes Ingress resource for my Linkding app. It will enable external HTTP access to the service (defined above). Since k3s uses Traefik as the ingress controller, this will route requests to <code>linkding.local</code> to the internal <code>linkding</code> Service on port 9090. I've specified <code>path: /</code> to make sure all HTTP requests to <code>linkding.local</code> get forwarded to the <code>linkding</code> Service. In short, this allows me to access my Linkding app locally on my network by the specified hostname.</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#ingressyaml","title":"ingress.yaml","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: linkding\n  namespace: linkding\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\nspec:\n  entryPoints:\n    - web # use 'web' for HTTP only\n  rules:\n    - host: linkding.local\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: linkding\n              port:\n                number: 9090\n</code></pre> <p>Something I'd like to point out here regarding exposing multiple apps that all listen on the same container port. Inside Kubernetes, there is no port conflict because each Service is an isolated abstraction with its own virtual IP inside the cluster. But for it to work that way the following must be true:</p> <ol> <li>Each Service has a unique name.</li> <li>Each Ingress path or hostname is unique.</li> <li>Each Service is of type <code>ClusterIP</code>.</li> </ol>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#new-secretsyaml-file","title":"New <code>secrets.yaml</code> File","text":"<p>For this resource, you must have already deployed and configured the External Secrets Operator in your cluster. I've detailed how I have done this in the following posts:</p> <ul> <li>Secrets Management With External Secrets Operator and 1Password (part 1)</li> <li>Secrets Management With External Secrets Operator and 1Password (part 2)</li> </ul> <p>The file below defines an ExternalSecret resource for my Linkding app. It essentially instructs the External Secrets Operator to retrieve the secret keys LD_SUPERUSER_NAME and LD_SUPERUSER_PASSWORD from an external provider and then store them in a cluster Secret resource named linkding-container-env. The external provider is referenced in the <code>secretStoreRef</code> section. I've set this to refresh every 12 hours, but will likely change that to a shorter interval in the future.</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#secretsyaml","title":"secrets.yaml","text":"<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: linkding-container-env\n  namespace: linkding\nspec:\n  refreshInterval: 12h\n  secretStoreRef:\n    name: onep-store\n    kind: ClusterSecretStore\n  data:\n    - secretKey: LD_SUPERUSER_NAME\n      remoteRef:\n        key: \"Linkding/username\"\n    - secretKey: LD_SUPERUSER_PASSWORD\n      remoteRef:\n        key: \"Linkding/password\"\n</code></pre>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#updated-deploymentyaml-file","title":"Updated <code>deployment.yaml</code> File","text":"<p>The additions to this file enable the use of the secrets retrieved from the external provider configured in the previous file (<code>secrets.yaml</code>). </p> <p>To clarify, the Secret resource named linkding-container-env will contain two secrets: LD_SUPERUSER_NAME and LD_SUPERUSER_PASSWORD, with their respective values retrieved from the external provider.</p> <p>The new <code>envFrom:</code> block takes the key=value pairs from the referenced Secret resource and passes them as environment variables to the linkding container. This is why the secret keys match the environment variable options for the Linkding application.</p> <p>There is no need to worry about these variables being set every time a new linkding container is deployed. They will not overwrite an existing superuser if one already exist. These variables are only used when the database is empty, specifically during any initial startup. If there is an existing database, they are ignored.</p>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#deploymentyaml-updated-containers-section","title":"deployment.yaml (updated <code>containers</code> section)","text":"<pre><code>      containers:\n        - name: linkding\n          image: sissbruecker/linkding:1.41.0\n          ports:\n            - containerPort: 9090\n          securityContext:\n            allowPrivilegeEscalation: false\n          envFrom:\n            - secretRef:\n                name: linkding-container-env\n          volumeMounts:\n            - name: linkding-data\n              mountPath: \"/etc/linkding/data\"\n</code></pre>"},{"location":"2025/09/30/updating-linkding-deployment-with-cloudflare-tunnels/#wrap-up","title":"Wrap-Up","text":"<p>With these improvement, my Linkding deployment is now fully declarative, secure, and accessible on my local network. By integrating FluxCD, Kustomize, and the External Secrets Operator, I have streamlined the setup and ongoing management of my self-hosted service. This approach will also ensure that deployments are repeatable, credentials are managed securely, and the application is always available as intended.</p> <p>If you have questions or want to share your own experiences with GitOps and Kubernetes, feel free to reach out. Find me <code>@cyberwatchdoug</code> on most places.</p>"},{"location":"2025/10/03/from-proxmox-to-kubernetes---evolving-my-homelab-part-3/","title":"From Proxmox to Kubernetes - Evolving My Homelab (part 3)","text":"<p>This is part 3 of my series detailing a transition of my Homelab architecture to using Kubernetes with Proxmox. You can check out the previous parts here: </p> <ul> <li>From Proxmox to Kubernetes - Evolving My Homelab (part 1)</li> <li>From Proxmox to Kubernetes - Evolving My Homelab (part 2)</li> </ul> <p>In part 3, I\u2019ll explore my implementation strategy for Kubernetes and how it has transformed my homelab.</p>"},{"location":"2025/10/03/from-proxmox-to-kubernetes---evolving-my-homelab-part-3/#the-strategic-shift-to-kubernetes","title":"The Strategic Shift to Kubernetes","text":"<p>Kubernetes has become the logical next step for my homelab. I've pointed this out briefly in parts 1 and 2. I enjoy the break-and-fix cycle of learning in my homelab. However, over the years I have found more joy with making things work consistently, from the very start, every time.</p> <p>My LXC-heavy setup worked as intended, but whenever something broke, I had to manually trace the problem and fix it which was always a great learning process, but not ideal long-term. It was fun, but not ideal, especially long-term, for my sanity! Adopting Kubernetes also aligns my homelab practices with modern production environments, thereby bridging my hobby and learning platform with career skills.</p> <p>Kubernetes provides a layer of orchestration, in addition to being a strong industry-standard practice in enterprise environments. That second part was the ultimate driver to this decision. That said, the first part, namely orchestration, was what ignited the idea for me. I could strategically design and architect my containers as deployments, where they would continue on through failures or updates, without my constant intervention and diagnostic microscope. My deployments would themselves manage replicasets and pods for me.</p> <p>If a deployment had an update and I wanted it rolled out, I'd make that change to a manifest, and Kubernetes would perform the update without any downtime to the service. That's not always the case, but the orchestration of that update was what I wanted. And the idea that everything could just be torn down and redeployed just as it was originally is exceedingly gratifying to me.</p>"},{"location":"2025/10/03/from-proxmox-to-kubernetes---evolving-my-homelab-part-3/#enhanced-automation-and-service-mesh-capabilities","title":"Enhanced Automation and Service Mesh Capabilities","text":"<p>Since I was already using Ansible to some degree to automate configurations, it made sense to continue that practice. Although I haven't completed it yet, the end goal with Ansible is to have a complete repository to be used for consistent configurations across my Kubernetes cluster. The initial focus will be on security related configurations, such as firewall rules, SELinux, and permissions and accounts. However, eventually I want a complete bootstrap process where I could run a few simple Ansible commands to get a brand new cluster deployed and configured.</p> <p>Most people don't necessarily think about the Kubernetes service mesh. In short, it's a platform of additional capabilities to manage and secure service-to-service communication within the cluster. It handles internal (East-West) traffic between services, providing load balancing, security, and observability features that complement traditional ingress (North-South) routing. The key here is that it's for control and data layers. This communication also includes metrics and logging. What I like best is the security functionality it can provide. Enforcing policies for service access and encryption.</p>"},{"location":"2025/10/03/from-proxmox-to-kubernetes---evolving-my-homelab-part-3/#addressing-the-complexity-trade-off","title":"Addressing the complexity trade-off","text":"<p>Adopting Kubernetes in a homelab environment introduces substantial complexity and setup overhead. Setting up manifests, learning new abstractions, and integrating with existing tools I was already using like Ansible can be daunting at first. However, the long-term benefits far outweigh the up-front investment in time and learning. Automated recovery, seamless updates, improved security, and the ability to scale or redeploy services with minimal effort to name a few. By embracing this complexity early, I\u2019m building a foundation that will make future growth, experimentation, and maintenance much more manageable and resilient.</p>"},{"location":"2025/10/14/quick-wins-by-reading-api-reference/","title":"Quick Wins by Reading API Reference","text":"<p>I know it can be tedious and more of a time sink to read through API reference or schema. But it is absolutely worth it, especially if you also take notes for yourself.</p> <p>Let me explain. I have been wrapping my head around how I can get up and running with using CloudNativePG for my databases. Starting with Linkding, but have plans to expand it to a half dozen other self-hosted apps.</p> <p>I didn't want to just copy someone else's manifest, or ask GenAI to create a manifest for me. I took the time to read what each field was for, and went down the rabbit hole reading first the <code>Cluster</code> reference, then <code>ClusterSpec</code> and several deeper fields within that.</p> <p>It took time as I made notes in Obsidian, with references back to the specific API schema pages. But now I've got a deeper understanding, and have plenty of notes to write about for places like this and other social media outlets.</p> <p>CloudNativePG is amazing with their API Reference documentation, as is Kubernetes. I know that's not the case for all of them. But I wanted to point out how beneficial it can be if you take the time to read through it and make your own notes.</p>"},{"location":"2025/10/21/building-confidence-in-uncomfortable-conversations/","title":"Building Confidence in Uncomfortable Conversations","text":"<p>I've never been great at uncomfortable conversations. Not at work, and not at home.</p> <p>There was always something in me that zeroed in on the uncomfortable aspects and amplified my anxiety. I'm sure there are plenty of explanations I could research to understand why but that's not the aim of this post (maybe another time).</p> <p>Every tough discussion felt like a minefield and I couldn't help but want to avoid them altogether. As I got older, and wiser, I learned that the difficult, challenging parts of life are where real growth happens. I began exploring ways to practice and overcome this tendency.</p> <p>One important thing I learned that was surprising, was that you can build confidence in discomfort. Through consistent practice and by meeting the challenge, I started to feel confidence in myself during these difficult, anxiety-producing discussions. I no longer wanted to avoid them; instead, I reframed them and focused on the purpose of the conversation.</p> <p>The lightbulb moment came when I stopped fixating on how awkward or tense things felt and began to focus on the purpose. Most of the time it's about trust; other times it's about clarity or mutual understanding.</p> <p>You see, the goal is never to \"win\" the moment, but to navigate it with honesty and respect.</p> <p>That mindset shift changed everything for me. I stopped avoiding the minefield and began appreciating them as opportunities to grow as a communicator and a leader.</p> <p>Quick practical steps I would love to tell my younger self: - Reframe: identify the conversation's purpose before you start. - Prepare: plan a concise opening and one desired outcome. - Practice: role-play with a friend (or AI), or rehearse aloud, then reflect afterwards.</p> <p>Learning never ends, but now every uncomfortable moment feels like progress and not pain.</p>"},{"location":"2025/10/23/perspective-shift---sme-to-advisor/","title":"Perspective Shift - SME to Advisor","text":"<p>Early in my cybersecurity journey, I believed my value lay in finding vulnerabilities, maintaining detection capabilities, and keeping adversaries at bay. The work was enjoyable, and it let me focus on technical skills while building a strong foundation for problem solving and analytical thinking.</p> <p>I was effective in those roles, but while studying for my CISSP exam I realized I wasn't thinking broadly enough. My aspirations weren't just to be another SME in cybersecurity, I wanted to lead teams and mentor others. To do that, I had to develop more than the technical skills alone.</p> <p>The moment that solidified this shift came during a Teams meeting with executive leadership, when I was asked not just where our defenses were strong, but why our overall approach made sense for the organization.</p> <p>I've come to realize the following - Leadership demands wearing two hats:</p> <ol> <li>Cybersecurity expert</li> <li>Business advisor</li> </ol> <p>Defending an organization isn't just about patches, identity management, or firewall rules (each of which continues to be vital).</p> <p>It's about guiding teams and senior leaders through decisions where security and business are not at odds but are intertwined.</p> <p>It's the ability to translate risk, resilience, and opportunity into terms that matter to each stakeholder. In short, it's seeing beyond the technical and building the whole puzzle. </p> <p>A simple practice I've adopted and has helped me: when speaking with non-technical stakeholders, answer why this matters first, then explain how it works. That small shift makes conversations less uncomfortable and more productive.</p> <p>That's when security leaders make the biggest impact, and it's something I keep in mind each day.</p>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"archive/2024/","title":"2024","text":""},{"location":"category/leadership/","title":"Leadership","text":""},{"location":"category/kubernetes/","title":"Kubernetes","text":""},{"location":"category/postgresql/","title":"Postgresql","text":""},{"location":"category/homelab/","title":"Homelab","text":""},{"location":"category/fluxcd/","title":"FluxCD","text":""},{"location":"category/ansible/","title":"Ansible","text":""},{"location":"category/proxmox/","title":"Proxmox","text":""},{"location":"category/cloudflare/","title":"Cloudflare","text":""},{"location":"category/security/","title":"Security","text":""},{"location":"category/1password/","title":"1Password","text":""},{"location":"category/k3s/","title":"k3s","text":""},{"location":"category/linux/","title":"Linux","text":""},{"location":"category/automation/","title":"Automation","text":""},{"location":"category/n8n/","title":"n8n","text":""},{"location":"category/syncthing/","title":"Syncthing","text":""},{"location":"category/obsidian/","title":"Obsidian","text":""},{"location":"category/identity/","title":"Identity","text":""},{"location":"category/icam/","title":"ICAM","text":""},{"location":"category/paperless-ngx/","title":"Paperless-ngx","text":""},{"location":"category/azurecli/","title":"AzureCLI","text":""},{"location":"category/azure/","title":"Azure","text":""},{"location":"category/dns/","title":"DNS","text":""},{"location":"page/2/","title":"Home","text":""},{"location":"archive/2025/page/2/","title":"2025","text":""}]}